---
title: "Predicting Exercise Type"
output: html_document
---

Load the Training and Testing Data provided for the exercise

``` {r load}

file_dest_training <- "pml-training.csv"

file_dest_testing <- "pml-testing.csv"

df_training <- read.csv(file_dest_training, na.strings=c("NA",""), header=TRUE)

colnames(df_training)

colnames_train <- colnames(df_training)

dim(df_training)

df_testing <- read.csv(file_dest_testing, na.strings=c("NA",""), header=TRUE)

colnames(df_testing)

colnames_test <- colnames(df_testing)

dim(df_testing)

```

Training Data is quite large whereas Testing data has only 20 rows.

To clean up the data we should first exclude some redundant columns and columns with NULLs

First 7 columns are not important for predictions.

Some other columns which has maximum values as NULLs will also be removed

``` {r NULL}

nonNAs <- function(x) {
    as.vector(apply(x, 2, function(x) length(which(!is.na(x)))))
}

colcnts <- nonNAs(df_training)

drops <- c()

for (cnt in 1:length(colcnts)) {
    if (colcnts[cnt] < nrow(df_training)) {
        drops <- c(drops, colnames_train[cnt])
    }
}

df_training <- df_training[,!(names(df_training) %in% drops)]

df_training <- df_training[,8:length(colnames(df_training))]

df_testing <- df_testing[,!(names(df_testing) %in% drops)]

df_testing <- df_testing[,8:length(colnames(df_testing))]

```

Data looks much cleaner. We can now build a model on the data set.

As still number of rows in Training data is pretty large, we will break it down into 4 datasets. 1 dataset we will use as Testing and remaining 3 will be used in training 3 separate models.


``` {r train}

library(caret)

set.seed(12321)

inTest = createDataPartition(y = df_training$classe, p = 0.25, list = FALSE)

Test.Train = df_training[inTest,]

Train.rem = df_training[-inTest,]

set.seed(12321)

inTrain1 = createDataPartition(y = Train.rem$classe, p = 0.33, list = FALSE)

Train1 = Train.rem[inTrain1,]

Train.rem2 = Train.rem[-inTrain1,]

set.seed(12321)
inTrain2 = createDataPartition(y = Train.rem2$classe, p = 0.50, list = FALSE)

Train2 = Train.rem2[inTrain2,]

Train3 = Train.rem2[-inTrain2,]

```

As per course instructions, we will go ahead with RandomForest method in caret package.

We wil first test if preprocessing of data gives some benefit in predictions.

We will use 5 fold cross validation to avoid overfitting of data

We will center and scale our variables in preprocessing

``` {r prepro}

set.seed(12321)

missClass = function(values, prediction) {
    sum(prediction != values)/length(values)
}

modFit1 = train(classe ~ ., method = 'rf', trControl = trainControl(method= 'cv', number = 5), data = Train1, importance = TRUE)

print(modFit1, digits=3)

set.seed(12321)

modFit2 = train(classe ~ ., method = 'rf', preProcess=c("center", "scale")  ,trControl = trainControl(method= 'cv', number = 5), data = Train1, importance = TRUE)

print(modFit2, digits=3)

predictions1 = predict(modFit1, Test.Train)

print(confusionMatrix(predictions1, Test.Train$classe), digits=4)

errRate1 = missClass(Test.Train$classe, predictions1)

errRate1

predictions2 = predict(modFit2, Test.Train)

print(confusionMatrix(predictions2, Test.Train$classe), digits=4)

errRate2 = missClass(Test.Train$classe, predictions2)

errRate2

```

### OOB error rates for both models is around 2%

Since Model with preprocessing is giving better accuracy, we will go ahead with preprocessing.

Based on 3 Data sets created as training, we will create 3 models with RandomForests, which can give us different predictions

``` {r model}

set.seed(12321)
rf.Fit1 = train(classe ~ ., method = 'rf', preProcess=c("center", "scale")  ,trControl = trainControl(method= 'cv', number = 5), data = Train1)

print(rf.Fit1, digits=3)



set.seed(12321)
rf.Fit2 = train(classe ~ ., method = 'rf', preProcess=c("center", "scale")  ,trControl = trainControl(method= 'cv', number = 5), data = Train2)

print(rf.Fit2, digits=3)


set.seed(12321)
rf.Fit3 = train(classe ~ ., method = 'rf', preProcess=c("center", "scale")  ,trControl = trainControl(method= 'cv', number = 5), data = Train3)

print(rf.Fit3, digits=3)

```

Based on 3 models created, we will predict the 20 testing rows for each of them

``` {r test}
predict(rf.Fit1, df_testing)

predict(rf.Fit2, df_testing)

predict(rf.Fit3, df_testing)

```

All but 2 prediction classe are same for all models. For rest of 2, we will take a majority vote of 2 vs. 1 from these predictions.

All results were uploaded on Coursera with 100% accuracy